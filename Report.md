# Deep Reinforcement Learning Project #1 - Report
Here below are reported the main characteristics of the Reinforcement Learning algorithm used to solve the DRLND <em>Navigation</em> environment.

## Learning Algorithm
The used algorithm is the Deep Q-Network (DQN). I started from the implementation of the algorithm provided by Udacity in its DRLND.

The `dqn` function in `my_methods.py` runs the selected number of episodes. Along each episode the agent performs a series of actions `action = agent.act(state, eps)` on the environment and the results (`next_state` and `reward`) are collected with the `action` taken and the actual `state` in the reply buffer (`class ReplayBuffer`) of the agent.
The actions are generated by the Deep Neural Network (DNN) model in `model.py` and some degree of randomness is included to ensure exploration for the training purpose. The randomness is managed by the parameter `eps` which decay during the training to reduce this feature along the training.

At each step, if the number of agent's experiences collected in the replay buffer is bit enough, the agent randomly retrieves the SARS (state, action, reward, next state) tuples from the replay buffer and update the DNN model to maximize the rewards (`Agent.learn` function). A target DNN is used beside a soft-update strategy (`Agent.soft_update`). This configuration is an important feature of the DWN algorithm that improve the stability of the learning process.

When an average score over 100 episodes higher than 13 is achieved an any improvement of the average score from this point is done, the DNN model is saved.

More details about DQN can be found in this [pdf](https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf).

### Improvements
Few variations of the DQN implementation are included in the agent. They can be managed with the `rainbow` dictionary, which includes:
- `'DDQN'` (Double Deep Q-Learning)[https://arxiv.org/abs/1509.06461]: decoupling the evaluation of the Q function for the action taken just by using the target network.
- `'Dueling'` (Dueling networks)[https://arxiv.org/abs/1511.06581]: the `Duel_DQN` model is used in place of the standard `DQN` in `model.py`. Here the DNN is split in two parts, one in charge of the state-value estimation and the other calculating the advantages for each action. 

## Model and Parameters
The file `model.py` holds the classes of the Neural Network for the generic DQN and the Dueling DQN.
The networks are built out of two hidden layer. The first with 64 nodes and the second with 32 nodes. The input layer has the extension of the state space and the output layer as the dimension of the action space.
In the case of the Dueling Network Architectures, the DNN for the evaluation of the state-value end with one single output node.
The state-value is combined with the advantage actions with the mean operator. 

Here a summary of parameters used:
  Learning Rate: `LR` = 5e-4,
  Soft Update `TAU` = 1e-3,
  `GAMMA` = 0.99,
  Update frequency `UPDATE_EVERY` = 4

  Replay Buffer
    size: `BUFFER_SIZE` = 10000,
    minibatch size: `BATCH_SIZE` = 64
  
  Epsilon-greedy:
    Starting Epsilon `eps_start` = 1.0
    Minimum Epsilon `eps_end` = 0.01
    Epsilon decay for episode `eps_decay` = 0.995

## Results

The trend below shows the score avarege on 100 episode achieved by the agent across the learning process.
The trend compares the DQN Vanilla with the improved methods implemented. a second trend shows the variance of the results. 
![Results](results/Training_Process.png)

All the methods overcome the target of average reward (over 100 episodes) of +13 and they stabilize around +16. This limit is due to the duration of the episode and the variability of the result is related to the stocastic distruburion of the bananas and the residual noise necessary to commit to exploration during training.

Running the trained model, the agent achieves easily 20 points in one single episode. 

## ToDo list
Much more stuff can be done to around the project.

#### Training Algorithm improvements
This Deep Q-Learning algorithm can be still improved with proved extensions.
Here the planned implementations:
1.Prioritized replay
2.Distributional DQN

#### Deep Analysis of the Parameters and Architecture
Wide analysis of the impact of the training parameters and model architecture on the training performance. A specific Jupyter Notebook will be set up for the purpose.

#### Improve Exploration Strategy
The exploration of the environment for part of the agent is an important aspect of the training, particularly when the environment is not stable. The randomness of the action taken to promote exploration can be improved with more sophisticated strategies taking into account the learning process. A more complex environment may be needed for testing.

#### Step to Pixel Based State Space
Use the *Navigation_Pixels.ipynb* and adapt the agent code to solve the banana collection environment using raw pixels. That will require mainly the modification of the `model.py` to include convolutional layers.
